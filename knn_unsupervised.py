# -*- coding: utf-8 -*-
"""KNN_Unsupervised.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EKE_6KkinSTxBU90hz0ET7hpPiNYZqqU

### **Reference Information & Description**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""## Data Ingestion"""

fp = r"/content/drive/MyDrive/ONLINE EDUCATION SYSTEM REVIEW.csv"

df = pd.read_csv(fp)

#Checking head of data set to make sure it read correctly
df.head()

#Checking head of data set to make sure it read correctly
df.tail()

#Checking how many rows and columns we have

df.shape

#1033 rows
#23 Columns

#Now we will use df info to get more information about the dataset

df.info()

#Check How many categorical vs numerical columns we have
df.dtypes.value_counts()

#13 numerical columns

#10 Categorical Columns

#Now we will use the describe method to statistically compare each column in the tabel that is numerical

df.describe()

"""# Visualizing Dataset"""

#Visualize the dataset correlation and put it into a heat map

fig, ax = plt.subplots(figsize=(10,10))

sns.heatmap(df.corr().abs(), vmax=1, square=True,annot=True, cmap='crest',ax=ax)

plt.title("Correlation between HR dataset")

plt.show()

#Since we have alot of columns
#Lets visualize the numerical columns using a histogram

columns = list(df)[:22]

df[columns].hist(figsize=(15,60),layout=(14,4))

plt.show()

#Now lets visualize the categorical variables


df["Gender"].value_counts().plot(kind='pie',ylabel='frequency',autopct='%1.1f%%')

#Now lets do home location

df["Home Location"].value_counts().plot(kind='pie',ylabel='frequency',autopct='%1.1f%%')

#Next we'll do level of education

df["Level of Education"].value_counts().plot(kind='pie',ylabel='frequency',autopct='%1.1f%%')

#We'll do Device type used to attend classes

df["Device type used to attend classes"].value_counts().plot(kind='bar',ylabel='frequency')

#Economic Status

df["Economic status"].value_counts().plot(kind='bar',ylabel='frequency')

#Are you involved in any sports?
df["Are you involved in any sports?"].value_counts().plot(kind='bar',ylabel='frequency')

#Do elderly people monitor you?
df["Do elderly people monitor you?"].value_counts().plot(kind='pie',ylabel='frequency',autopct='%1.1f%%')

#Have separate room for studying?
df["Have separate room for studying?"].value_counts().plot(kind='pie',ylabel='frequency',autopct='%1.1f%%')

#Engaged in group studies?
df["Engaged in group studies?"].value_counts().plot(kind='bar',ylabel='frequency')

#Average marks scored before pandemic in traditional classroom
df["Average marks scored before pandemic in traditional classroom"].value_counts().plot(kind='bar',ylabel='frequency')

#Interested in?
df["Interested in?"].value_counts().plot(kind='bar',ylabel='frequency')

#Your level of satisfaction in Online Education
df["Your level of satisfaction in Online Education"].value_counts().plot(kind='pie',ylabel='frequency',autopct='%1.1f%%')

"""# **Data Preproccesing**"""

#Check data types

df.dtypes

#Check null values

df.isna().sum()

#There are no null values in this dataset so we can move forward

#Dropped my input variable(Y)
df = df.drop('Your level of satisfaction in Online Education', axis=1)
df.columns

#Now we will call the get dummies method
#The gert dummies method One Hot Encodes
#This means that it creates a numerical representation for categorical variables
#dropfirst = True drops a column because of multicolineary effect
df = pd.get_dummies(df,columns=df.columns,drop_first=True)

df

"""# **Shuffle Dataset**"""

from sklearn.utils import shuffle
#Shuffling the data means to randomly reorder therows of your data
#This is done to introduce randomness and remve any patternsthat might exist in the data.
#Shuffling is commonly applied when splitting the dataset into test and traing sets

df = shuffle(df)

df

"""# **Normalize or Standardize Dataset**"""

from sklearn.preprocessing import MinMaxScaler
scaler_m = MinMaxScaler()

df = scaler_m.fit_transform(df.values)

df.shape

"""# **Invoke Elbow Method**"""

from sklearn.cluster import KMeans

# Within-Cluster Sum of Squares

wss = []
for i in range(1,10):
  km = KMeans(n_clusters = i, init= 'k-means++',random_state=42)
  km.fit(df)
  wss.append(km.inertia_)

plt.plot(range(1,10),wss)
plt.title('The Elbow Method')
plt.xlabel('The Number of Clusters')
plt.ylabel('Sum of squared Distances')
plt.show()

from yellowbrick.cluster import KElbowVisualizer
model = KMeans()
visualizer = KElbowVisualizer(model,k=(1,10),timings=False)
visualizer.fit(df)
visualizer.show()

"""# **Find the Number of Clusters Using the Silhoutte Method**"""

from yellowbrick.cluster.silhouette import silhouette_visualizer
from yellowbrick.cluster.elbow import silhouette_score
#If the silhouette score is high and close to 1 then it means that the clusters are well separated and distinct which is good.
#Basically you want to choose the cluster that gives you the highest silhouette score.

for i in range (2,10):
  km = KMeans(n_clusters = i, max_iter=100)
  km.fit(df)
  score = silhouette_score(df,km.labels_)
  print('For Cluster {}, the Silhoutte Score is {}'.format(i,score))

"""# **Plot Graph of Silhoutte Values**"""

silhoutte_coefficients = []
for i in range(2,10):
  km = KMeans(n_clusters = i, max_iter=100)
  km.fit(df)
  score = silhouette_score(df, km.labels_)
  silhoutte_coefficients.append (score)

plt.plot(range(2,10),silhoutte_coefficients)
plt.xticks(range(2,10))
plt.xlabel('Number of Clusters')
plt.ylabel('Silhoutte Coefficient')
plt.show()

"""# **Transform the Data using PCA**"""

from sklearn.decomposition import PCA
pca = PCA()
X = pca.fit_transform(df)

"""# **Initialize the Number of Clusters of Kmeans**"""

km = KMeans(n_clusters=2)
label = km.fit_predict(X)
unique_labels = np.unique(label)

"""# **Display the Clusters Using Scatter Plot**"""

for i in unique_labels:
  plt.scatter(X[label==i,0], X[label==i,1], label=i)

plt.legend()
plt.title("Online School Groups")
plt.show()

